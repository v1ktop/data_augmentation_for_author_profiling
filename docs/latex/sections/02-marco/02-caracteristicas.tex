\section{Extracción de características}
El preprocesamiento y la extracción de características, son pasos muy importantes en la clasificación de textos, en las siguientes secciones se presentan algunas de las técnicas más empleadas y se mencionan dos métodos de representación de características: la bolsa de palabras y las representaciones distribucionales.

\subsection{Preprocesamiento}
Dependiendo de la tarea de clasificación, algunos elementos pueden desecharse para enfocar nuestra atención en los elementos más informativos. Algunos de estos elementos son: \textit{palabras de paro}, errores gramaticales, signos de puntuación, etc. Además de esto, el texto extraído de redes sociales contiene enlaces del Internet, menciones de usuario, etiquetas (conocidos como hashtags), emoticonos y un vocabulario muy informal (p.e. abreviaturas no convencionales). A continuación, se explican brevemente algunas técnicas empleadas para el limpiado y preprocesamiento de textos.

\begin{enumerate}
\item \textbf{Tokenización}: Es un método de preprocesamiento en el cual se divide una cadena de caracteres en palabras, frases, símbolos y otros elementos dentro del texto, llamados \textit{tokens} \citep{kowsari2019text}. Se pueden utilizar diferentes algoritmos, para este proceso, lo más simple es separar el texto mediante un espacio o carácter común, por ejemplo:

Texto original: \textit{``Los días de verano son calurosos''}.

Los tokens del texto anterior son los siguientes: \{``Los'',  ``días'', ``de'', ``verano'', ``son'', ``calurosos''\}'

\item \textbf{Palabras de paro}: Son palabras con mayor frecuencia en los documentos, y por lo tanto poco útiles para la discriminación entre documentos de diferentes clases. Ejemplos de ellas son: \{``\textit{a}'' ``\textit{the}'', ``\textit{they}'', ``\textit{he}'' , ``\textit{she}'', ...\} (para el idioma inglés). En algunas tareas de clasificación de textos, las palabras de paro no son de importancia y lo más común es removerlas de los documentos. Nothman \citep{nothman2018stop} presenta un análisis de las palabras de paro y su impacto en la clasificación de textos.

\item \textbf{Capitalización}: Los textos contienen diversas formas de capitalización de palabras para formar una oración. Dado que los documentos consisten en muchas oraciones, una capitalización diversa puede ser muy problemática en la clasificación de textos largos. La técnica más común para tratar la capitalización inconsistente es reducir cada palabra a minúsculas \citep{kowsari2019text}.

\item \textbf{Reducción de ruido}: La mayoría de los textos contienen caracteres innecesarios para la clasificación de documentos, como signos de puntuación o caracteres especiales. En tareas como detección de autoría pueden ser útiles, pero en general agregan ruido a los modelos de clasificación de textos.

\item \textbf{Lematización}:  Es el proceso de convertir palabras a su forma base o raíz (e.g., morfema base del significado) \citep{kamath2019deep}. La desventaja es que este método requiere un diccionario y tablas de búsqueda \citep{kamath2019deep}. Uno de los algoritmos más populares es el algoritmo de Porter \citep{porter2001snowball}, el cual hace una lematización por fuerza bruta mediante el truncamiento de las palabras.

\item  \textbf{Otras técnicas}: Adicionalmente a las técnicas descritas, también es posible preprocesar los textos para intentar normalizarlos, con la intención de facilitar al clasificador la identificación de patrones. Algunas de ellas son: la corrección de errores ortográficos, enmascaramiento de textos, etiquetado de partes de la oración, etc.

\end{enumerate}


\subsection{N-Gramas}
Es una técnica para extraer características para representar un texto, los n-gramas son un conjunto de palabras o caracteres que respetan el orden de aparición en el texto; el número $n$ indica la longitud de la secuencia a considerar, lo más común es utilizar valores de $n$ pequeños (uni-gramas, bi-gramas, tri-gramas) \citep{kowsari2019text}. 

\textbf{Ejemplo de bi-gramas}: 

Texto Original: \textit{``Con el tiempo todo pasa''}

Bi-gramas:\{``con el", ``el tiempo", ``tiempo todo", ``todo pasa"\}


\subsection{Bolsa de palabras BoW}
El modelo de bolsa de palabras o BoW (por sus siglas en inglés ``Bag of Words'') es una representación simplificada de un texto. Normalmente, se utiliza un criterio o pesado específico, como lo puede ser la frecuencia de cada palabra, para representar cada texto. Es decir, en el modelo BoW, el conjunto de documentos es representado mediante una matriz de pesos, siendo las columnas palabras únicas del conjunto de datos y las filas representan un documento. Este modelo es muy simple, donde no es posible capturar el orden secuencial de las palabras -como en una oración o un documento-, con lo que las relaciones semánticas entre las palabras se pierden. Sin embargo, en este modelo las palabras capturan el contenido de un documento y esta representación puede ser utilizada para determinar el tema principal del documento \citep{kowsari2019text}.


\subsubsection{Pesado de palabras} La forma más básica de pesado de características es mediante el pesado \textit{TF} (\textit{term frequency} por sus siglas en inglés), el cual consiste en contar el número de ocurrencias de cada término $t$ en el documento $d$. Los métodos basados en \textit{TF} generalmente consisten en representar la frecuencia de palabras como un peso escalado o normalizado, aunque es fácil de implementar y muy intuitivo, este método es limitado porque las palabras más comunes pueden dominar la representación.

\subsubsection{TF-IDF Term Frequency-Inverse Document Frequency} Esta técnica de pesado fue propuesta por \citep{jones1972statistical}, con el objetivo de mitigar el efecto de las palabras más comunes del corpus. \textit{IDF} asigna menos peso a aquellas palabras presentes en la mayoría de los documentos en la colección y, por lo tanto, estas palabras no se consideran útiles para identificar patrones discriminatorios. La representación matemática del peso de un término en un documento por \textit{TF-IDF} está dada en la ecuación \ref{eq:tf_idf}

\input{sections/equation/tf_idf}

Donde $N$ es el número total de documentos en la colección y $df(t)$ es el número de documentos que contienen el término $t$, el primer término $TF(d,t)$ es la frecuencia del término $t$ en el documento $d$. Aunque \textit{TF-IDF} trata de solucionar el problema de términos comunes en el documento, sigue sufriendo de otras limitaciones. Un problema común es que \textit{TF-IDF} no se puede utilizar para medir la similitud entre palabras en el documento, debido a que las representaciones son generadas a nivel documento.

\subsection{Representaciones distribucionales}
Su objetivo principal es capturar el significado semántico de las palabras, donde cada palabra del vocabulario es representada mediante un vector n-dimensional de números reales. Recientemente \citep{mikolov2013distributed} presentó el modelo Word2Vec, para generar vectores de palabras, el cual tiene dos algoritmos: el modelo CBOW y el modelo Skip-gram. CBOW  predice la palabra central del contexto que la rodea, mientras que Skip-gram hace lo contrario y predice la distribución (probabilidad) de las palabras de contexto de una palabra central. Word2Vec proporciona una herramienta muy poderosa para descubrir relaciones entre los textos de un corpus, así como la similitud entre palabras.

Glove es un modelo similar a Word2Vec, fue propuesto por \citep{pennington2014glove} y su objetivo principal de capturar contextos globales combinando factorización de matrices y ocurrencias locales. Este modelo demostró ser más rápido en entrenamiento y mejora en tareas como analogía (en comparación con Word2Vec). Los autores generaron diferentes modelos preentrenados para su libre acceso, estos modelos fueron entrenados con grandes conjuntos de datos como CRAWL\footnote{www.commoncrawl.org} y Wikipedia\footnote{www.wikipedia.org}.

Otro modelo distribucional es FastText, este modelo fue desarrollado por el laboratorio de inteligencia artificial de Facebook \citep{mikolov2017advances}. A diferencia de los modelos anteriores este modelo considera la morfología de las palabras, cada vector es enriquecido con una bolsa de vectores de caracteres de n-gramas que es derivada de una matriz de coocurrencia. La principal ventaja de este modelo es su habilidad de obtener vectores para palabras fuera del vocabulario. Los vectores preentrenados están disponibles en la página oficial de FastText\footnote{www.fasttext.cc}  y la última liberación fue un modelo entrenado en 157 idiomas \citep{grave2018learning}.

Para un mayor detalle de la teoría y el cálculo de este tipo de representaciones consultar el capítulo 5 del libro de \citep{kamath2019deep}.

\subsection{Representaciones contextuales}

Este tipo de representación difiere de las representaciones distribucionales, en que a cada token le es asignado a una representación que es una función de la secuencia completa de entrada, por lo tanto, cada token tendrá un vector de características diferentes de acuerdo con la secuencia de entrada. A continuación, se describen los trabajos más relevantes bajo este paradigma. 
ELMo fue presentado por \citep{peters2018deep}, la idea original es que los vectores resultantes sean aprendidos por una red bidireccional LSTM (ver sección 2.4.2). El objetivo principal de este modelo, es representar el uso complejo de las palabras (e.g., sinonimia y semántica) y como varían estos usos en diferentes contextos lingüísticos (i.e., polisemia). Los autores demostraron que estas representaciones pueden ser añadidas a modelos existentes y mejorar significativamente el estado del arte en tareas como: respuesta de preguntas, implicación textual y análisis de sentimientos.

BERT es un modelo de representación de lenguaje \cite{devlin2018bert}, fue diseñado para preentrenar representaciones bidireccionales profundas condicionando y uniendo el contexto de la secuencia de entrada de derecha a izquierda en todas las capas. Como resultado, el modelo preentrenado puede ser ajustado a la tarea deseada agregando una capa de salida adicional. Este modelo obtuvo nuevos resultados del estado del arte en once tareas de procesamiento de lenguaje natural, incluyendo el conjunto de tareas GLUE \cite{wang2018glue}.