{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Input, Activation, Dropout, Bidirectional\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "import keras.backend as K\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(le, y_test, y_pred):\n",
    "        print(\"Classifation Report\")\n",
    "\n",
    "        target_names = le.classes_\n",
    "        class_indices = {cls: idx for idx, cls in enumerate(le.classes_)}\n",
    "        print(class_indices)\n",
    "\n",
    "        print(metrics.classification_report(y_test, y_pred, target_names=target_names,\n",
    "                                            labels=[\"\" + str(class_indices[cls]) for cls in target_names]))\n",
    "\n",
    "        print(\"============================================================\")\n",
    "        print(\"Confusion matrix\")\n",
    "        print(\"============================================================\")\n",
    "        print(target_names)\n",
    "        print(confusion_matrix(y_test, y_pred,\n",
    "            labels=[class_indices[cls] for cls in target_names]))\n",
    "\n",
    "        precision_micro, recall_micro, fscore_micro, _ = \\\n",
    "        precision_recall_fscore_support(y_test, y_pred, average='micro', pos_label=None)\n",
    "\n",
    "        precisions_macro, recalls_macro, fscore_macro, _ = \\\n",
    "        precision_recall_fscore_support(y_test, y_pred, average='macro', pos_label=None)\n",
    "\n",
    "        precisions_weighted, recalls_weighted, fscore_weighted, _ =\\\n",
    "        precision_recall_fscore_support(y_test, y_pred, average='weighted', pos_label=None)\n",
    "\n",
    "        measures = \"p: %.4f r: %.4f f1: %.4f\"\n",
    "        print(\"Micro:  \" + measures % (precision_micro, recall_micro, fscore_micro))\n",
    "        print(\"Macro:  \" + measures % (precisions_macro, recalls_macro, fscore_macro))\n",
    "        print(\"Weight: \" + measures % (precisions_weighted, recalls_weighted, fscore_weighted))\n",
    "\n",
    "        print('Test Accuracy: %.4f' % accuracy_score(y_test, y_pred))\n",
    "        #print('ROC AUC: %.3f' % roc_auc_score(y_true=y_test, y_score=y_pred))\n",
    "        print(\"============================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "#from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_texts_from_dir(cat_dir):\n",
    "    texts = []\n",
    "    f_names = []\n",
    "    DIR = cat_dir\n",
    "\n",
    "    for f_name in sorted(os.listdir(DIR)):\n",
    "        f_path = os.path.join(DIR, f_name)\n",
    "        f = open(f_path, \"r\")\n",
    "        texts += [f.read()]\n",
    "        f.close()\n",
    "    print(\"%d files loaded from %s\" % (len(texts), cat_dir))\n",
    "    return texts\n",
    "\n",
    "def get_texts_from_dir2(cat_dir, truth_file_path):\n",
    "    texts = []\n",
    "    f_names = []\n",
    "    DIR = cat_dir\n",
    "    f_truth = open(truth_file_path)\n",
    "    dict_truth = {}\n",
    "    for line in f_truth:\n",
    "        line = line.strip().split(\" \")\n",
    "        dict_truth[line[0]] = (line[1])\n",
    "    f_truth.close()\n",
    "    \n",
    "    categories = []\n",
    "    for f_name in sorted(os.listdir(DIR)):\n",
    "        f_path = os.path.join(DIR, f_name)\n",
    "        f = open(f_path, \"r\")\n",
    "        texts += [f.read()]\n",
    "        f_names += [f_name]\n",
    "        categories += [dict_truth[f_name[:-4]]]\n",
    "        f.close()\n",
    "    print(\"%d files loaded from %s\" % (len(texts), cat_dir))\n",
    "    return f_names, texts, categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        \"\"\"Recall metric.\n",
    "\n",
    "        Only computes a batch-wise average of recall.\n",
    "\n",
    "        Computes the recall, a metric for multi-label classification of\n",
    "        how many relevant items are selected.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        \"\"\"Precision metric.\n",
    "\n",
    "        Only computes a batch-wise average of precision.\n",
    "\n",
    "        Computes the precision, a metric for multi-label classification of\n",
    "        how many selected items are relevant.\n",
    "        \"\"\"\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 files loaded from ./2017_txt/positive/\n",
      "403 files loaded from ./2017_txt/negative/\n",
      "401 files loaded from ./2017_txt/test/\n",
      "820 files loaded from ./2018_txt/\n",
      "<class 'list'> <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "tr_txt = get_texts_from_dir(\"./2017_txt/positive/\")\n",
    "tr_txt2 = get_texts_from_dir(\"./2017_txt/negative/\")\n",
    "\n",
    "f_names, X_test, y_test = get_texts_from_dir2(\"./2017_txt/test/\",\"./test_golden_truth2017.txt\")\n",
    "\n",
    "f_names2, X_test_2019, y_test_2019 = get_texts_from_dir2(\"./2018_txt/\",\"./test_golden_truth2018.txt\")\n",
    "\n",
    "n_cl1 = len(tr_txt)\n",
    "n_cl2 = len(tr_txt2)\n",
    "\n",
    "labpos = []\n",
    "labneg = []\n",
    "\n",
    "for i in range(n_cl1):\n",
    "    labpos.append(1)\n",
    "for i in range(n_cl2):\n",
    "    labneg.append(0)\n",
    "print(type(tr_txt),type(tr_txt2))\n",
    "\n",
    "#print(f2_names)\n",
    "#X_train = tr_txt + tr_txt2\n",
    "X_train = tr_txt + tr_txt2\n",
    "#y_train = np.concatenate((labpos, labneg), axis=0)\n",
    "y_train = np.concatenate((labpos, labneg), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128413\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 437587 unique tokens.\n",
      "Frequency of \"hello\" is 185\n",
      "Found 703550 unique tokens.\n",
      "Frequency of \"hello\" is 380\n",
      "Found 1294563 unique tokens.\n",
      "Frequency of \"hello\" is 982\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Build Tokenizer and Vocabulary\n",
    "tokenizer = Tokenizer(num_words=2100, filters=\"\", lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "# Dictionary of the WHOLE extracted vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# Frequencies of words\n",
    "word_counts = tokenizer.word_counts\n",
    "print(\"Frequency of \\\"%s\\\" is %s\" % (\"hello\", word_counts[\"hello\"]))\n",
    "\n",
    "tokenizer.fit_on_texts(X_test)\n",
    "# Dictionary of the WHOLE extracted vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# Frequencies of words\n",
    "word_counts = tokenizer.word_counts\n",
    "print(\"Frequency of \\\"%s\\\" is %s\" % (\"hello\", word_counts[\"hello\"]))\n",
    "\n",
    "tokenizer.fit_on_texts(X_test_2019)\n",
    "# Dictionary of the WHOLE extracted vocabulary\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "# Frequencies of words\n",
    "word_counts = tokenizer.word_counts\n",
    "print(\"Frequency of \\\"%s\\\" is %s\" % (\"hello\", word_counts[\"hello\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have\n",
      "1294563\n"
     ]
    }
   ],
   "source": [
    "inverted_word_index = {}\n",
    "for word in word_index.keys():\n",
    "    inverted_word_index[word_index[word]]=word\n",
    "    \n",
    "print(inverted_word_index[20])\n",
    "top_words = len(inverted_word_index)\n",
    "print(len(inverted_word_index))\n",
    "\n",
    "max_features = len(inverted_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "486it [00:04, 84.62it/s] \n",
      "9it [00:00, 83.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n",
      "486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "401it [00:03, 115.86it/s]\n",
      "8it [00:00, 75.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39503\n",
      "401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "820it [00:08, 99.29it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# integer encode documents\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "X_train1 = []\n",
    "result = []\n",
    "for i,doc in tqdm(enumerate(X_train)):\n",
    "    words = list(text_to_word_sequence(doc))\n",
    "    for w in words:\n",
    "        if w in word_index:\n",
    "            result.append(word_index[w])\n",
    "        else:\n",
    "            c = 0\n",
    "    X_train1.append(result)\n",
    "    result = []\n",
    "    \n",
    "print(len(max(X_train1)))\n",
    "print(len(X_train1))\n",
    "\n",
    "X_test1 = []\n",
    "result = []\n",
    "for i,doc in tqdm(enumerate(X_test)):\n",
    "    words = list(text_to_word_sequence(doc))\n",
    "    for w in words:\n",
    "        if w in word_index:\n",
    "            result.append(word_index[w])\n",
    "        else:\n",
    "            c = 0\n",
    "    X_test1.append(result)\n",
    "    result = []\n",
    "    \n",
    "print(len(max(X_test1)))\n",
    "print(len(X_test1))\n",
    "\n",
    "##\n",
    "X_test1_2019 = []\n",
    "result = []\n",
    "for i,doc in tqdm(enumerate(X_test_2019)):\n",
    "    words = list(text_to_word_sequence(doc))\n",
    "    for w in words:\n",
    "        if w in word_index:\n",
    "            result.append(word_index[w])\n",
    "        else:\n",
    "            c = 0\n",
    "    X_test1_2019.append(result)\n",
    "    result = []\n",
    "    \n",
    "print(len(max(X_test1_2019)))\n",
    "print(len(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'>\n",
      "<class 'int'>\n",
      "<class 'numpy.int64'>\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "y_test = list(map(int, y_test))\n",
    "\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))\n",
    "\n",
    "##\n",
    "y_test_2019 = list(map(int, y_test_2019))\n",
    "\n",
    "print(type(y_train[0]))\n",
    "print(type(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from collections import deque\n",
    "\n",
    "def sliding_window(iterable, size=25, step=1, fillvalue=0): #list, size of window, step, fill empty value\n",
    "    if size < 0 or step < 1: raise ValueError\n",
    "    it = iter(iterable)\n",
    "    q = deque(islice(it, size), maxlen=size)\n",
    "    if not q: return  # empty iterable or size == 0\n",
    "    q.extend(fillvalue for _ in range(size - len(q)))  # pad to size\n",
    "    while True:\n",
    "        yield iter(q)  # iter() to avoid accidental outside modifications\n",
    "        q.append(next(it))\n",
    "        q.extend(next(it, fillvalue) for _ in range(step - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/hermit/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: generator 'sliding_window' raised StopIteration\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "486it [00:02, 193.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252110\n",
      "252110\n"
     ]
    }
   ],
   "source": [
    "X_train2 = []\n",
    "y_train2 = []\n",
    "\n",
    "sequence_len = 35\n",
    "\n",
    "for X_array,y in tqdm(zip(X_train1,y_train)):\n",
    "    #print(X_array)\n",
    "    window = sliding_window(X_array,size=sequence_len,step=sequence_len)\n",
    "\n",
    "    for i, w in (enumerate(window)):\n",
    "        l = list(w)\n",
    "        l = np.asarray(l)\n",
    "        #print(l)\n",
    "        X_train2.append(l)\n",
    "        y_train2.append(int(y))\n",
    "X_train2 = np.asarray(X_train2)        \n",
    "print(len(X_train2))\n",
    "print(len(y_train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/hermit/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: generator 'sliding_window' raised StopIteration\n",
      "  import sys\n",
      "401it [00:02, 201.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217606\n",
      "217606\n"
     ]
    }
   ],
   "source": [
    "X_test2 = []\n",
    "y_test2 = []\n",
    "for X_array,y in tqdm(zip(X_test1,y_test)):\n",
    "    #print(X_array)\n",
    "    window = sliding_window(X_array,size=sequence_len,step=sequence_len)\n",
    "\n",
    "    for i, w in (enumerate(window)):\n",
    "        l = list(w)\n",
    "        l = np.asarray(l)\n",
    "        #print(l)\n",
    "        X_test2.append(l)\n",
    "        y_test2.append(int(y))\n",
    "X_test2 = np.asarray(X_test2)        \n",
    "print(len(X_test2))\n",
    "print(len(y_test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [[    1   145   582 ... 28556    14     1]\n",
      " [73604  5115 13775 ...     3  3326    14]\n",
      " [17670    66    20 ...   440    13    55]\n",
      " ...\n",
      " [   14   132  3322 ... 25044    71 89174]\n",
      " [  362 67462     1 ...  2046   748     3]\n",
      " [ 5025   437    83 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train2[0]), X_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import FastText as fText\n",
    "\n",
    "# Creating the model\n",
    "embedding_model = KeyedVectors.load_word2vec_format('word2vec-vectors-300.bin',binary=True)\n",
    "\n",
    "#with open('wiki.en', 'rb') as f:\n",
    "    #embedding_model = pickle.load(f)\n",
    "#print(embedding_model[\"hello\"])\n",
    "\n",
    "#embedding_model = KeyedVectors.load_word2vec_format('GNvectors.bin', binary=True)\n",
    "embedding_weights = {key: embedding_model[word] if word in embedding_model else\n",
    "                          np.random.uniform(-0.25, 0.25, 300)\n",
    "                     for key, word in inverted_word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 300)         388368900 \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 388,689,901\n",
      "Trainable params: 321,001\n",
      "Non-trainable params: 388,368,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 300\n",
    "weights = np.array([embedding_weights[i+1] for i in range(len(embedding_weights))])\n",
    "class_weight = {0: 1.,\n",
    "                1: 10.}\n",
    "\n",
    "#print(weights.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=None, trainable=False, weights=[weights]))\n",
    "model.add(Bidirectional(LSTM(100)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1731532718178137627\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10663444480\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 10309501553859979247\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:07:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 252110 samples, validate on 217606 samples\n",
      "Epoch 1/15\n",
      "252110/252110 [==============================] - 105s 416us/step - loss: 1.2945 - acc: 0.5042 - val_loss: 0.7836 - val_acc: 0.4757\n",
      "Epoch 2/15\n",
      "252110/252110 [==============================] - 103s 409us/step - loss: 1.2416 - acc: 0.5413 - val_loss: 0.8091 - val_acc: 0.4929\n",
      "Epoch 3/15\n",
      "252110/252110 [==============================] - 104s 412us/step - loss: 1.1823 - acc: 0.5795 - val_loss: 0.7805 - val_acc: 0.5271\n",
      "Epoch 4/15\n",
      "252110/252110 [==============================] - 103s 410us/step - loss: 1.1181 - acc: 0.6131 - val_loss: 0.7324 - val_acc: 0.5770\n",
      "Epoch 5/15\n",
      "252110/252110 [==============================] - 103s 409us/step - loss: 1.0614 - acc: 0.6395 - val_loss: 0.6723 - val_acc: 0.6229\n",
      "Epoch 6/15\n",
      "252110/252110 [==============================] - 104s 411us/step - loss: 1.0052 - acc: 0.6642 - val_loss: 0.7516 - val_acc: 0.5852\n",
      "Epoch 7/15\n",
      "252110/252110 [==============================] - 103s 409us/step - loss: 0.9460 - acc: 0.6897 - val_loss: 0.8021 - val_acc: 0.5468\n",
      "Epoch 8/15\n",
      "252110/252110 [==============================] - 103s 408us/step - loss: 0.8815 - acc: 0.7138 - val_loss: 0.9230 - val_acc: 0.5328\n",
      "Epoch 9/15\n",
      "252110/252110 [==============================] - 104s 411us/step - loss: 0.8154 - acc: 0.7396 - val_loss: 0.8130 - val_acc: 0.5856\n",
      "Epoch 10/15\n",
      "252110/252110 [==============================] - 104s 413us/step - loss: 0.7445 - acc: 0.7655 - val_loss: 0.7073 - val_acc: 0.6468\n",
      "Epoch 11/15\n",
      "252110/252110 [==============================] - 104s 412us/step - loss: 0.6841 - acc: 0.7898 - val_loss: 0.7633 - val_acc: 0.6555\n",
      "Epoch 12/15\n",
      "252110/252110 [==============================] - 104s 411us/step - loss: 0.6220 - acc: 0.8117 - val_loss: 0.8462 - val_acc: 0.6427\n",
      "Epoch 13/15\n",
      "252110/252110 [==============================] - 103s 410us/step - loss: 0.5712 - acc: 0.8301 - val_loss: 0.7908 - val_acc: 0.6853\n",
      "Epoch 14/15\n",
      "252110/252110 [==============================] - 103s 408us/step - loss: 0.5259 - acc: 0.8467 - val_loss: 0.7432 - val_acc: 0.7186\n",
      "Epoch 15/15\n",
      "252110/252110 [==============================] - 103s 408us/step - loss: 0.4781 - acc: 0.8632 - val_loss: 0.7653 - val_acc: 0.7167\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f70328913c8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train2, y_train2, validation_data=(X_test2, y_test2), epochs=15, batch_size=250,class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/820 [00:00<?, ?it/s]/home/hermit/anaconda3/envs/py36/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: generator 'sliding_window' raised StopIteration\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "100%|██████████| 820/820 [06:14<00:00,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_prediction = []\n",
    "def most_frequent(List): \n",
    "    return max(set(List), key = List.count) \n",
    "\n",
    "for X_array in tqdm(X_test1_2019):\n",
    "    #print(X_array)\n",
    "    window = sliding_window(X_array,size=sequence_len,step=sequence_len)\n",
    "    \n",
    "    X_test3 = []\n",
    "    for i, w in (enumerate(window)):\n",
    "        l = list(w)\n",
    "        l = np.asarray(l)\n",
    "        #print(l)\n",
    "        X_test3.append(l)   \n",
    "        \n",
    "    X_test3 = np.asarray(X_test3)\n",
    "    predictions = model.predict(X_test3, verbose=0)\n",
    "    \n",
    "    results = []\n",
    "    for j in predictions:\n",
    "        if j >= 0.5:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    \n",
    "    y_p = most_frequent(results)\n",
    "    y_prediction.append(y_p)\n",
    "print(len(y_prediction),len(y_test_2019))\n",
    "#print(len(X_test2))\n",
    "#print(len(y_test2))\n",
    "\n",
    "#predictions = model.predict(X_test1, verbose=0)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifation Report\n",
      "{'0': 0, '1': 1}\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.96      0.90      0.93       741\n",
      "          1       0.40      0.61      0.48        79\n",
      "\n",
      "avg / total       0.90      0.88      0.89       820\n",
      "\n",
      "============================================================\n",
      "Confusion matrix\n",
      "============================================================\n",
      "['0' '1']\n",
      "[[670  71]\n",
      " [ 31  48]]\n",
      "Micro:  p: 0.8756 r: 0.8756 f1: 0.8756\n",
      "Macro:  p: 0.6796 r: 0.7559 f1: 0.7071\n",
      "Weight: p: 0.9026 r: 0.8756 f1: 0.8864\n",
      "Test Accuracy: 0.8756\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hermit/anaconda3/envs/py36/lib/python3.6/site-packages/numpy/lib/arraysetops.py:518: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask &= (ar1 != a)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_te = label_encoder.fit_transform(\" \".join(map(str, y_test_2019)).split())\n",
    "report(label_encoder, y_te, y_prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
